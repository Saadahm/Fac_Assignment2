{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUgqbi1FpC6DnORLyhoehJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadahm/Fac_Assignment2/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW318pfJCeH"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DecEcoBSJKMY"
      },
      "source": [
        "is_ipython = 'inline'  in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHxqORSSJMsK"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=input_size,out_features=64)\n",
        "    self.fc2 = nn.Linear(in_features=64,out_features=8)\n",
        "    self.out = nn.Linear(in_features=8,out_features=3)\n",
        "  def forward(self,t):\n",
        "    t = t.flatten(start_dim=1)\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t  \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2BkAmBJMyE"
      },
      "source": [
        "Experience = namedtuple('Experience',('state','action','next_state','reward'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToEWVTAJTp-"
      },
      "source": [
        "class ReplayMemory():\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "    self.push_count =0\n",
        "  def push(self,experience):   \n",
        "    if len(self.memory)<self.capacity:   #check if it's less than the memory's capacity\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count%self.capacity] = experience   #store in front of memory\n",
        "      self.push_count+=1\n",
        "  def sample(self,batch_size):\n",
        "    return random.sample(self.memory,batch_size)\n",
        "  def can_provide_sample(self,batch_size):\n",
        "   # print(len(self.memory))\n",
        "    return len(self.memory)>=batch_size  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBc5_NwbJVyy"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self,start,end,decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "  def get_exploration_rate(self,current_step):\n",
        "    return self.end + (self.start-self.end)*math.exp(-1.*current_step*self.decay)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1omflFaiJYZ5"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self,strategy,num_actions,device):\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.device = device \n",
        "  def select_action(self,state,policy_net):\n",
        "    rate = strategy.get_exploration_rate(self.current_step)\n",
        "    self.current_step +=1\n",
        "    if rate> random.random():\n",
        "      action =  random.randrange(self.num_actions)  # explore\n",
        "      return torch.tensor([action]).to(device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        #print(state,\"f1\")\n",
        "        return policy_net(state).argmax(dim=1).to(device) # exploit"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNzLIvk9JdO5"
      },
      "source": [
        "\n",
        "def formatPrice(n):\n",
        "    return(\"-Rs.\" if n<0 else \"Rs.\")+\"{0:.2f}\".format(abs(n))\n",
        "def getStockDataVec():\n",
        "    vec = []\n",
        "    lines = open(\"/content/NFLX.csv\",\"r\").read().splitlines()\n",
        "    for line in lines[1:2267]:\n",
        "        #print(line)\n",
        "        #print(float(line.split(\",\")[4]))\n",
        "        vec.append(float(line.split(\",\")[4]))\n",
        "        #print(vec)\n",
        "    return vec \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def getState(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(sigmoid(block[i + 1] - block[i]))\n",
        "    return np.array([res])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AG6JqREL8Nz"
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "    plt.plot(get_moving_average(moving_avg_period, values))\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython: display.clear_output(wait=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsKUn4yMAdi"
      },
      "source": [
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpO_L14MIYa"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "\n",
        "    t1 = torch.cat(batch.state)\n",
        "   # print(t1)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.reward)\n",
        "    t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRDQoTIWxi-"
      },
      "source": [
        "class QValues():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def get_current(policy_net, states, actions):\n",
        "      return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "    def get_next(target_net, next_states):\n",
        "      final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "      non_final_state_locations = (final_state_locations == False)\n",
        "      non_final_states = next_states[non_final_state_locations]\n",
        "      batch_size = next_states.shape[0]\n",
        "      values = torch.zeros(batch_size)#.to(QValues.device)\n",
        "      values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "      return values  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P71-1ogEMN-G"
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update  = 10\n",
        "memory_size = 100000\n",
        "lr = 0.01\n",
        "num_episodes =30\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67wezQmMP0h"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#em = CartPoleEnvManager(device)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, 3, device)\n",
        "memory = ReplayMemory(memory_size)\n",
        "vec = getStockDataVec()\n",
        "k=len(vec)\n",
        "#print(k)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aagr4VUHMSH9"
      },
      "source": [
        "\n",
        "policy_net = DQN(64).to(device)\n",
        "target_net = DQN(64).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "window_size = 64"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-sXNPFNTpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53764d84-a2bd-46f0-baf6-52c083712f61"
      },
      "source": [
        "episode_durations = []\n",
        "for episode in range(num_episodes):\n",
        "    print(\"Episode \" + str(episode) + \"/\" + str(num_episodes))\n",
        "    state = torch.tensor(getState(vec, 0, window_size + 1))\n",
        "    total_profit = 0\n",
        "    invent = []\n",
        "    #state = getState(vec,)\n",
        "    max_transaction = 20 \n",
        "    total_money = 10000\n",
        "    c_s_h = 0\n",
        "    c_t_c =0\n",
        "    for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(1,action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "         #print(\"Buy: \" + formatPrice(data[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "        reward = -vec[t]*b_p[0]+b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        experiences = memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "        current_q_values = QValues.get_current(policy_net, states.float(), actions)\n",
        "        next_q_values = QValues.get_next(target_net, next_states.float())\n",
        "        target_q_values = (next_q_values * gamma) + rewards\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        # episode_durations.append(t)\n",
        "        # plot(episode_durations, 100)\n",
        "         break\n",
        "    print(total_profit)    \n",
        "    if episode % target_update == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0/30\n",
            "1626.8692140374299\n",
            "Episode 1/30\n",
            "18786.193615985663\n",
            "Episode 2/30\n",
            "-398.3303665239841\n",
            "Episode 3/30\n",
            "767.853038720577\n",
            "Episode 4/30\n",
            "187.6243945543332\n",
            "Episode 5/30\n",
            "391.3757589377793\n",
            "Episode 6/30\n",
            "941.3706344254714\n",
            "Episode 7/30\n",
            "271.8440943422371\n",
            "Episode 8/30\n",
            "242.83182035598276\n",
            "Episode 9/30\n",
            "166.079273524808\n",
            "Episode 10/30\n",
            "700.1820097697803\n",
            "Episode 11/30\n",
            "1327.2378470101062\n",
            "Episode 12/30\n",
            "1339.4146415006912\n",
            "Episode 13/30\n",
            "630.0245920322732\n",
            "Episode 14/30\n",
            "563.0609222507943\n",
            "Episode 15/30\n",
            "1426.8405725151274\n",
            "Episode 16/30\n",
            "855.7809444873155\n",
            "Episode 17/30\n",
            "238.72570507352867\n",
            "Episode 18/30\n",
            "652.747594919253\n",
            "Episode 19/30\n",
            "832.0578376462921\n",
            "Episode 20/30\n",
            "1231.630633643133\n",
            "Episode 21/30\n",
            "400.79044861580843\n",
            "Episode 22/30\n",
            "830.3409068477953\n",
            "Episode 23/30\n",
            "-216.45473484242882\n",
            "Episode 24/30\n",
            "986.8942163324358\n",
            "Episode 25/30\n",
            "-352.6177335465935\n",
            "Episode 26/30\n",
            "421.4434724193193\n",
            "Episode 27/30\n",
            "939.7288159104739\n",
            "Episode 28/30\n",
            "70.25979417169145\n",
            "Episode 29/30\n",
            "772.9204336462315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pToT-UfEO8pM",
        "outputId": "b7a06df5-c48b-4ae5-b7e1-42b3ab4e3472"
      },
      "source": [
        "print(total_profit)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "772.9204336462315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdXC5vgVTeNw",
        "outputId": "57181e69-9d1f-418b-c384-ef2a07aef23e"
      },
      "source": [
        "max_transaction = 50\n",
        "total_money = 10000\n",
        "c_s_h = 0\n",
        "c_t_c =0\n",
        "print(k)\n",
        "for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "        print(\"Buy: \" + formatPrice(vec[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        print('profit :', total_profit)\n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action ==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "       # print('f1')\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      \n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        #episode_durations.append(timestep)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "print(total_profit)\n",
        "print(total_money)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2265\n",
            "Buy: Rs.21.83\n",
            "profit : 776.7691049886673\n",
            "Buy: Rs.42.16\n",
            "profit : 774.348746358996\n",
            "Buy: Rs.50.43\n",
            "profit : 785.6071780600861\n",
            "Buy: Rs.46.26\n",
            "profit : 801.0750607625718\n",
            "Buy: Rs.63.48\n",
            "profit : 805.4035269827482\n",
            "Buy: Rs.79.50\n",
            "profit : 808.6665548814029\n",
            "Buy: Rs.126.45\n",
            "profit : 804.0157463669166\n",
            "Buy: Rs.110.13\n",
            "Buy: Rs.104.21\n",
            "profit : 796.455774243932\n",
            "profit : 810.0089690832581\n",
            "Buy: Rs.97.32\n",
            "profit : 827.5006544288749\n",
            "Buy: Rs.120.67\n",
            "profit : 824.0510842951255\n",
            "Buy: Rs.94.53\n",
            "Buy: Rs.98.00\n",
            "profit : 835.087874463531\n",
            "profit : 841.3646988898013\n",
            "Buy: Rs.99.59\n",
            "profit : 844.6403110135385\n",
            "Buy: Rs.102.19\n",
            "profit : 844.7191599926812\n",
            "Buy: Rs.93.56\n",
            "profit : 847.5826706092247\n",
            "Buy: Rs.87.97\n",
            "profit : 855.6222486924544\n",
            "Buy: Rs.94.37\n",
            "profit : 861.3062469216832\n",
            "Buy: Rs.95.11\n",
            "profit : 862.9609458334376\n",
            "Buy: Rs.96.16\n",
            "profit : 862.352351466371\n",
            "Buy: Rs.124.58\n",
            "profit : 861.963601400872\n",
            "Buy: Rs.125.89\n",
            "profit : 867.6055876869648\n",
            "Buy: Rs.181.35\n",
            "Buy: Rs.184.45\n",
            "profit : 882.1227775839307\n",
            "profit : 896.9761199998841\n",
            "Buy: Rs.199.54\n",
            "profit : 895.7888768084981\n",
            "Buy: Rs.195.08\n",
            "profit : 896.4427067929303\n",
            "Buy: Rs.190.42\n",
            "profit : 892.8275492097898\n",
            "892.8275492097898\n",
            "10119.90711556356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Z3CRbHh4MWk4",
        "outputId": "c624b0d4-61bc-4119-bb46-9f4a451462a7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9881, 0.3682, 0.9483,  ..., 0.9742, 0.2405, 0.1431],\n",
            "        [0.4583, 0.4679, 0.4169,  ..., 0.2392, 0.5495, 0.7600],\n",
            "        [0.3765, 0.4075, 0.4986,  ..., 0.6251, 0.1755, 0.3177],\n",
            "        ...,\n",
            "        [0.2159, 0.1809, 0.7389,  ..., 0.9870, 0.2369, 0.1256],\n",
            "        [0.3705, 0.9816, 0.7341,  ..., 0.4804, 0.0373, 0.7325],\n",
            "        [0.4375, 0.4886, 0.5004,  ..., 0.4871, 0.5367, 0.5449]],\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-52532305a49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'QValues' has no attribute 'get_next'"
          ]
        }
      ]
    }
  ]
}